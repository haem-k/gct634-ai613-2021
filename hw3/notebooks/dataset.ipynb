{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dataset import MAESTRO_small\n",
    "from constants import HOP_SIZE"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset\n",
    "\n",
    "MAESTRO_small is a dataset class for piano transcription. You can specify the path of the maestro dataset through *path*. *groups* indicates the dataset splits. You can select some of the below and pass them as a list of strings.\n",
    "```\n",
    ">>train_set.available_groups()\n",
    "['train', 'validation', 'test', 'debug']\n",
    "```\n",
    "When you create the dataset, it will read all the files (audio, midi) and parse them. Here, we will use `'debug'` dataset, which is a subset of the `'train'` dataset (first 10 pieces)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "example_dataset = MAESTRO_small(path='../data', groups=['debug'], sequence_length=None, random_sample=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "example_dataset.available_groups()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data types\n",
    "You can treat it as a python list. When you access it with an index, `__getitem__` method will be called.\n",
    "It will return a python dict which contains the file paths, normalized audios, and piano rolls.\n",
    "\n",
    "Sample rate of the audio is `16000` and we will use `hop_size=512` for mel-spectrogram and piano roll, resulting in  frame rate of `16000/512 == 31.25`.\n",
    "\n",
    "In the example below, we examine the shapes of the tensors in the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = example_dataset[1]\n",
    "print(f'data: {data}')\n",
    "print(f'audio_shape: {data[\"audio\"].shape}')\n",
    "print(f'frame_roll_shape: {data[\"frame\"].shape}')\n",
    "print(f'onset_roll_shape: {data[\"onset\"].shape}')\n",
    "\n",
    "print(f'HOP_SIZE({HOP_SIZE}) x piano_roll length({data[\"frame\"].shape[0]}): {HOP_SIZE*data[\"frame\"].shape[0]}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Piano roll\n",
    "Piano roll is an array-like representation of notes.\n",
    "`frame` and `onset` are tensors of shape (*number of frames*, *number of pithces (88)*).\n",
    "\n",
    "The `frame` represents the sustains of the notes and `onset` marks the attacks of the notes.\n",
    "Let's suppose there is a piano note of pitch `p` pressed at time `t1` and released at `t2`.  \n",
    "Then, `frame[t1:t2+1, p] == 1` and `onset[t1, p] == 1`, and `0` everywhere else.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(311)\n",
    "plt.plot(data['audio'].numpy()[:400*HOP_SIZE])\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplot(312)\n",
    "plt.imshow(data['frame'].numpy()[:400].T, aspect='auto', origin='lower')\n",
    "plt.subplot(313)\n",
    "plt.imshow(data['onset'].numpy()[:400].T, aspect='auto', origin='lower')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### sequence_length & random_sample\n",
    "If we specify the `sequence_length`, the dataset trims the audio into the given length. When `random_sample=False`, the segments at the start of the audio will be used. If `random_sample=True`, the segments starting at random position will be used. We will use this attribute to make our training samples stocastic.\n",
    "\n",
    "if `sequence_length` is not divisible by `HOP_SIZE`, it will shorten the audio for proper adjustment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "example_dataset = MAESTRO_small(path='../data', groups=['debug'], sequence_length=10000, random_sample=True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = example_dataset[1]\n",
    "print(f'data path: {data[\"path\"]}')\n",
    "print(f'audio_shape: {data[\"audio\"].shape}')\n",
    "print(f'frame_roll_shape: {data[\"frame\"].shape}')\n",
    "print(f'onset_roll_shape: {data[\"onset\"].shape}')"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(311)\n",
    "plt.plot(data['audio'].numpy()[:400*HOP_SIZE])\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplot(312)\n",
    "plt.imshow(data['frame'].numpy()[:400].T, aspect='auto', origin='lower')\n",
    "plt.subplot(313)\n",
    "plt.imshow(data['onset'].numpy()[:400].T, aspect='auto', origin='lower')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a96d57b1bc12bd9a330f866f7a3118e9f4e5ad5921b76a2d6712d50bdc59d920"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}