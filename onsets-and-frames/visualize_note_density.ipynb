{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize prediction results\n",
    "\n",
    "Use debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pretty_midi\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import IPython.display as ipd\n",
    "\n",
    "SAMPLE_RATE = 44100\n",
    "MIN_MIDI = 21\n",
    "MAX_MIDI = 108\n",
    "\n",
    "HOP_SIZE = 512\n",
    "N_MELS = 229\n",
    "N_FFT = 2048\n",
    "F_MIN = 30\n",
    "F_MAX = 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytsmod as tsm\n",
    "\n",
    "# 절대경로로 써줄 것\n",
    "x, sr = sf.read('/home/haemin/Documents/gct634-ai613-2021/onsets-and-frames/data/MAESTRO/2015/MIDI-Unprocessed_R1_D1-1-8_mid--AUDIO-from_mp3_01_R1_2015_wav--5.wav')\n",
    "x = x.T\n",
    "\n",
    "assert sr == SAMPLE_RATE\n",
    "\n",
    "print(x.shape)\n",
    "print(sr)\n",
    "ipd.Audio(x, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_length = x.shape[-1]  # length of the audio sequence x.\n",
    "\n",
    "s_fixed = 2  # stretch the audio signal 1.3x times.\n",
    "s_ap = np.array([[0, x_length / 2, x_length], [0, x_length, x_length * 1.5]])  # double the first half of the audio only and preserve the other half.\n",
    "\n",
    "# Scale given audio file\n",
    "x_s_fixed = tsm.wsola(x, s_fixed)\n",
    "x_s_ap = tsm.wsola(x, s_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_s_fixed.shape) # (2, 13814616)\n",
    "print(x_s_ap.shape)    # (2, 15939941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio(x_s_fixed, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ipd.Audio(x_s_ap, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.subplot(311).set_title('Original .wav')\n",
    "# librosa.display.waveplot(x, sr=sr, color='r', alpha=0.5)\n",
    "librosa.display.waveplot(x[0], sr=sr, color='r')\n",
    "# plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "plt.subplot(312).set_title('Cropped to same length as original .wav')\n",
    "# librosa.display.waveplot(x_s_fixed[:, :x_length], sr=sr, color='g', alpha=0.5)\n",
    "librosa.display.waveplot(x_s_fixed[0, :x_length], sr=sr, color='g')\n",
    "# plt.autoscale(enable=True, axis='x', tight=True)\n",
    "\n",
    "plt.subplot(313).set_title('Total time-scaled .wav')\n",
    "librosa.display.waveplot(x_s_fixed[0], sr=sr)\n",
    "# plt.autoscale(enable=True, axis='x', tight=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Mel-spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.abs(librosa.stft(x[0], n_fft=N_FFT, hop_length=HOP_SIZE))**2   # Power spectrum is the default  \n",
    "mel_S = librosa.feature.melspectrogram(S=S, sr=sr, n_mels=N_MELS) # 128 mel bins is the default\n",
    "#S = librosa.feature.melspectrogram(y, sr=sr, n_mels=128) # it is possible to compute mel-spec directly from the waveform\n",
    "\n",
    "S2 = np.abs(librosa.stft(x_s_fixed[0, :x_length], n_fft=N_FFT, hop_length=HOP_SIZE))**2   # Power spectrum is the default  \n",
    "mel_S2 = librosa.feature.melspectrogram(S=S2, sr=sr, n_mels=N_MELS) # 128 mel bins is the defaul\n",
    "\n",
    "S3 = np.abs(librosa.stft(x_s_fixed[0], n_fft=N_FFT, hop_length=HOP_SIZE))**2   # Power spectrum is the default  \n",
    "mel_S3 = librosa.feature.melspectrogram(S=S3, sr=sr, n_mels=N_MELS) # 128 mel bins is the defaul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.subplot(311).set_title('Original Mel Spectogram')\n",
    "S_dB1 = librosa.power_to_db(mel_S, ref=np.max)\n",
    "img1 = librosa.display.specshow(S_dB1, x_axis='time', y_axis='mel', sr=sr)\n",
    "\n",
    "plt.subplot(312).set_title('Cropped to same length as original .wav')\n",
    "S_dB2 = librosa.power_to_db(mel_S2, ref=np.max)\n",
    "img2 = librosa.display.specshow(S_dB2, x_axis='time', y_axis='mel', sr=sr)\n",
    "\n",
    "plt.subplot(313).set_title('Total time-scaled .wav')\n",
    "S_dB3 = librosa.power_to_db(mel_S3, ref=np.max)\n",
    "img3 = librosa.display.specshow(S_dB3, x_axis='time', y_axis='mel', sr=sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Note Density\n",
    "\n",
    "Test with single segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Segment the input audio\n",
    "segment_length = SAMPLE_RATE * 5\n",
    "segment_time = segment_length // SAMPLE_RATE\n",
    "\n",
    "frames_per_sec = SAMPLE_RATE // HOP_SIZE\n",
    "frames_per_segment = segment_length // HOP_SIZE    # Total number of frame in a segment\n",
    "\n",
    "frame_begin = 0\n",
    "frame_end = frames_per_segment\n",
    "\n",
    "begin = frame_begin * HOP_SIZE\n",
    "end = begin + segment_length\n",
    "\n",
    "# Cut out first 1 sec for this particular music bc of noise\n",
    "x = x[:, SAMPLE_RATE:]\n",
    "print(x.shape)\n",
    "\n",
    "audio_seg = x[0, begin:end]    # x shape: (2, 11637117)\n",
    "\n",
    "print(f'segment_length: {segment_length}')\n",
    "print(f'frames_per_sec: {frames_per_sec}')\n",
    "print(f'frames_per_segment: {frames_per_segment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_seg, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Settings for measuring\n",
    "peak_setting = {}\n",
    "peak_setting['pre_max'] = 3\n",
    "peak_setting['post_max'] = 3\n",
    "peak_setting['pre_avg'] = 3\n",
    "peak_setting['post_avg'] = 5\n",
    "peak_setting['delta'] = 0.7\n",
    "peak_setting['wait'] = 5\n",
    "\n",
    "# Measure number of onsets\n",
    "onset_env = librosa.onset.onset_strength(audio_seg, sr=SAMPLE_RATE)\n",
    "peaks = librosa.util.peak_pick(onset_env, peak_setting['pre_max'], peak_setting['post_max'], peak_setting['pre_avg'], peak_setting['post_avg'], peak_setting['delta'], peak_setting['wait'])\n",
    "times = librosa.times_like(onset_env, sr=SAMPLE_RATE)\n",
    "# segment_onset = librosa.onset.onset_detect(audio_seg, sr=SAMPLE_RATE)\n",
    "print(peaks.shape)\n",
    "print(peaks)\n",
    "print(times[peaks])\n",
    "\n",
    "# Visualize onsets\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(times, onset_env, label='Onset strength', alpha=0.5)\n",
    "plt.vlines(times[peaks], 0, onset_env.max(), color='r',\n",
    "           linestyle='--', label='Onsets')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute note_density\n",
    "num_onsets = len(times[peaks])\n",
    "note_density = num_onsets / segment_time\n",
    "print(f'note_density: {note_density}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note density for all segments in a audio file\n",
    "\n",
    "Retrieve every segments from a given audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of segments in a audio file\n",
    "num_segments = len(x[0]) // segment_length\n",
    "\n",
    "# Keep track of all note density and time\n",
    "all_note_density = np.zeros((num_segments,))\n",
    "time_labels = []\n",
    "\n",
    "for i in range(num_segments):\n",
    "    begin = i * segment_length\n",
    "    end = begin + segment_length\n",
    "    \n",
    "    start_time = begin / SAMPLE_RATE\n",
    "    end_time = end / SAMPLE_RATE\n",
    "    time_labels.append(f'{int(start_time//60):02d}:{int(start_time%60):02d}-{int(end_time//60):02d}:{int(end_time%60):02d}')\n",
    "    \n",
    "    segment = x[0, begin:end]\n",
    "    \n",
    "    # Detect onsets\n",
    "    onset_env = librosa.onset.onset_strength(segment, sr=SAMPLE_RATE)\n",
    "    peaks = librosa.util.peak_pick(onset_env, peak_setting['pre_max'], peak_setting['post_max'], peak_setting['pre_avg'], peak_setting['post_avg'], peak_setting['delta'], peak_setting['wait'])\n",
    "    times = librosa.times_like(onset_env, sr=SAMPLE_RATE)\n",
    "    \n",
    "    num_onsets = len(times[peaks])\n",
    "    note_density = num_onsets / segment_time\n",
    "    all_note_density[i] = note_density\n",
    "    \n",
    "    if i == num_segments-6:\n",
    "        print(num_onsets)\n",
    "        print(note_density)\n",
    "        saved_segment = segment\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title('Onset Detection')\n",
    "        plt.plot(times, onset_env, label='Onset strength', alpha=0.5)\n",
    "        plt.vlines(times[peaks], 0, onset_env.max(), color='r', linestyle='--', label='Onsets')\n",
    "        plt.legend()\n",
    "        \n",
    "ipd.Audio(saved_segment, rate=SAMPLE_RATE)\n",
    "# print(all_note_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute time for each segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize note density with waveform\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "plt.title('Note density with waveform')\n",
    "\n",
    "plt.subplot(211).set_title('Waveform')\n",
    "librosa.display.waveplot(x[0], sr=sr)\n",
    "\n",
    "index_to_show = np.arange(0, num_segments, 4)\n",
    "time_labels_vis = [time_labels[index] for index in index_to_show]\n",
    "plt.subplot(212).set_title('Note Density')\n",
    "plt.bar(np.arange(num_segments), all_note_density, align='center', width=0.7)\n",
    "plt.xticks(index_to_show, time_labels_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply time-scale algorithm to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale audio segment by factor of 2\n",
    "scale = 2\n",
    "x_scaled = []\n",
    "\n",
    "for i in range(num_segments):\n",
    "    begin = i * segment_length\n",
    "    end = begin + segment_length\n",
    "    \n",
    "    start_time = begin / SAMPLE_RATE\n",
    "    end_time = end / SAMPLE_RATE\n",
    "    time_labels.append(f'{int(start_time//60):02d}:{int(start_time%60):02d}-{int(end_time//60):02d}:{int(end_time%60):02d}')\n",
    "    \n",
    "    segment = x[0, begin:end]\n",
    "    \n",
    "    # Scale in time if density is too high\n",
    "    if all_note_density[i] >= 8:\n",
    "        print(f'{time_labels[i]}\\tnote density = {all_note_density[i]}')\n",
    "        segment_scaled = tsm.wsola(segment, scale)\n",
    "        segment = segment_scaled\n",
    "#         # for listening purpose\n",
    "#         saved_segment = segment_scaled\n",
    "    \n",
    "    x_scaled.extend(segment)\n",
    "\n",
    "x_scaled = np.array(x_scaled)\n",
    "librosa.display.waveplot(saved_segment, sr=sr)\n",
    "# ipd.Audio(saved_segment, rate=SAMPLE_RATE)\n",
    "ipd.Audio(x_scaled, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(audio_path, midi_path):\n",
    "    \"\"\"Loads an audio track and the corresponding labels.\"\"\"\n",
    "    audio, sr = sf.read(audio_path, dtype='int16')\n",
    "    print(sr)\n",
    "    assert sr == SAMPLE_RATE\n",
    "    frames_per_sec = sr / HOP_SIZE\n",
    "\n",
    "    audio = torch.ShortTensor(audio)\n",
    "    audio_length = len(audio)\n",
    "\n",
    "    mel_length = audio_length // HOP_SIZE + 1\n",
    "\n",
    "    midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "    midi_length_sec = midi.get_end_time()\n",
    "    frame_length = min(int(midi_length_sec * frames_per_sec), mel_length)\n",
    "\n",
    "    audio = audio[:frame_length * HOP_SIZE]\n",
    "    frame = midi.get_piano_roll(fs=frames_per_sec)\n",
    "    onset = np.zeros_like(frame)\n",
    "    \n",
    "    for inst in midi.instruments:\n",
    "        print(inst.notes)\n",
    "        quit()\n",
    "        for note in inst.notes:\n",
    "            onset[note.pitch, int(note.start * frames_per_sec)] = 1\n",
    "    \n",
    "    # to shape (time, pitch (88))\n",
    "    frame = torch.from_numpy(frame[MIN_MIDI:MAX_MIDI + 1].T)\n",
    "    onset = torch.from_numpy(onset[MIN_MIDI:MAX_MIDI + 1].T)\n",
    "    data = dict(path=audio_path, audio=audio, frame=frame, onset=onset)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = sorted(glob.glob(\"\".join(['./data/sample/', '*.midi'])))\n",
    "wav_files = sorted(glob.glob(\"\".join(['./data/sample/', '*.wav'])))\n",
    "\n",
    "for i in range(len(wav_files)):\n",
    "    m_file = midi_files[i]\n",
    "    w_file = wav_files[i]\n",
    "    print(f'{m_file}\\n{w_file}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load(wav_files[0], midi_files[0])\n",
    "print(f'data path: {data[\"path\"]}')\n",
    "print(f'audio_shape: {data[\"audio\"].shape}')\n",
    "print(f'frame_roll_shape: {data[\"frame\"].shape}')\n",
    "print(f'onset_roll_shape: {data[\"onset\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(311).set_title('ONF Predicted test sample')\n",
    "plt.plot(data['audio'].numpy()[:segment_length,0])\n",
    "plt.autoscale(enable=True, axis='x', tight=True)\n",
    "plt.subplot(312)\n",
    "plt.imshow(data['frame'].numpy()[:segment_length].T, aspect='auto', origin='lower')\n",
    "plt.subplot(313)\n",
    "plt.imshow(data['onset'].numpy()[:segment_length].T, aspect='auto', origin='lower')\n",
    "\n",
    "print(data['audio'].shape)\n",
    "print(data['frame'].shape)\n",
    "print(data['onset'].shape)\n",
    "print(data['onset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PyTSMode to scale time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytsmod as tsm\n",
    "\n",
    "audio, sr = soundfile.read(wav_files[0], dtype='int16')\n",
    "x = x.T\n",
    "x_length = x.shape[-1]  # length of the audio sequence x.\n",
    "\n",
    "s_fixed = 1.3  # stretch the audio signal 1.3x times.\n",
    "s_ap = np.array([[0, x_length / 2, x_length], [0, x_length, x_length * 1.5]])  # double the first half of the audio only and preserve the other half.\n",
    "\n",
    "x_s_fixed = tsm.wsola(x, s_fixed)\n",
    "x_s_ap = tsm.wsola(x, s_ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get note density same as training code\n",
    "\n",
    "Note density = number of onsets in a segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_586499/2081053909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/haemin/Documents/gct634_final/data/MAESTRO/2015/MIDI-Unprocessed_R2_D1-2-3-6-7-8-11_mid--AUDIO-from_mp3_07_R2_2015_wav--2.flac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sf' is not defined"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "HOP_LENGTH = SAMPLE_RATE * 32 // 1000\n",
    "ONSET_LENGTH = SAMPLE_RATE * 32 // 1000\n",
    "OFFSET_LENGTH = SAMPLE_RATE * 32 // 1000\n",
    "HOPS_IN_ONSET = ONSET_LENGTH // HOP_LENGTH\n",
    "HOPS_IN_OFFSET = OFFSET_LENGTH // HOP_LENGTH\n",
    "MIN_MIDI = 21\n",
    "MAX_MIDI = 108\n",
    "\n",
    "N_MELS = 229\n",
    "MEL_FMIN = 30\n",
    "MEL_FMAX = SAMPLE_RATE // 2\n",
    "WINDOW_LENGTH = 2048\n",
    "\n",
    "SCALE = 1.3\n",
    "\n",
    "x, sr = sf.read('/home/haemin/Documents/gct634_final/data/MAESTRO/2015/MIDI-Unprocessed_R2_D1-2-3-6-7-8-11_mid--AUDIO-from_mp3_07_R2_2015_wav--2.flac', dtype='int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# Settings\n",
    "audio_length = len(x)\n",
    "sequence_length = HOP_LENGTH * 200\n",
    "segment_length = 100 * HOP_LENGTH\n",
    "\n",
    "frames_per_sec = SAMPLE_RATE // HOP_LENGTH\n",
    "frames_per_sequence = sequence_length // HOP_LENGTH    # Total number of frame\n",
    "\n",
    "# Settings for peak picking\n",
    "peak_setting = {}\n",
    "peak_setting['pre_max'] = 3\n",
    "peak_setting['post_max'] = 3\n",
    "peak_setting['pre_avg'] = 3\n",
    "peak_setting['post_avg'] = 5\n",
    "peak_setting['delta'] = 0.7\n",
    "peak_setting['wait'] = 5\n",
    "\n",
    "# Initialize random\n",
    "random = np.random.RandomState(seed=10)\n",
    "\n",
    "# Get index of start frame and end frame\n",
    "frame_begin = random.randint(audio_length - sequence_length) // HOP_LENGTH\n",
    "frame_end = frame_begin + frames_per_sequence\n",
    "\n",
    "begin = frame_begin * HOP_LENGTH\n",
    "end = begin + sequence_length\n",
    "\n",
    "audio = x[begin:end]    \n",
    "\n",
    "print(f'segment_length: {frames_per_sequence}')\n",
    "print(f'frames_per_sec: {frames_per_sec}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure note density for every 100 frames\n",
    "num_onset_threshold = 10\n",
    "num_segments = len(audio) // segment_length\n",
    "scaled_index = torch.full((num_segments,), -1)\n",
    "\n",
    "scaled_audio = []\n",
    "audio_segment = []\n",
    "for i in range(num_segments):\n",
    "    first_frame = i * segment_length               # 0~100, 100~200\n",
    "    last_frame = first_frame + segment_length\n",
    "\n",
    "    # Segment audio file\n",
    "    segment = audio[first_frame:last_frame].astype(np.float32)\n",
    "    audio_segment.append(segment)\n",
    "\n",
    "    # Detect onset \n",
    "    onset_env = librosa.onset.onset_strength(segment, sr=SAMPLE_RATE)\n",
    "    peaks = librosa.util.peak_pick(onset_env, peak_setting['pre_max'], peak_setting['post_max'], peak_setting['pre_avg'], peak_setting['post_avg'], peak_setting['delta'], peak_setting['wait'])\n",
    "    times = librosa.times_like(onset_env, sr=SAMPLE_RATE)\n",
    "    \n",
    "    # Visualize onsets\n",
    "    plt.plot(times, onset_env, label='Onset strength')\n",
    "    plt.vlines(times[peaks], 0, onset_env.max(), color='r', alpha=0.9,\n",
    "               linestyle='--', label='Onsets')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Stretch segment based on number of detected onsets\n",
    "    if len(peaks) == 0:\n",
    "        num_onsets = 0      # no peaks detected\n",
    "    else:\n",
    "        num_onsets = len(times[peaks])\n",
    "        print(num_onsets)\n",
    "\n",
    "    if num_onsets >= num_onset_threshold:\n",
    "        segment_scaled = tsm.wsola(segment, SCALE)\n",
    "        scaled_audio.extend(segment_scaled)\n",
    "        scaled_index[i] = first_frame // HOP_LENGTH\n",
    "    else:\n",
    "        scaled_audio.extend(segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_segment[0], rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio_segment[1], rate=SAMPLE_RATE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de6df30355d7931246ff51d35175e7b9e46c72bab5b480631d53a6fb9f6779c6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
